---
title: "Keyword Transformer: A Self-Attention Model for Keyword Spotting"
collection: publications
permalink: /publications/2021-kws
date: 2021-04-01
venue: 'Preprint. Submitted to INTERSPEECH2021'
paperurl: 'https://arxiv.org/abs/2104.00769'
citation: 'A. Berg, M. OConnor, and M. Tairum Cruz. "Keyword Transformer: A Self-Attention Model for Keyword Spotting." arXiv preprint arXiv:2104.00769 (2021).'
---

In this paper, we apply the popular Transformer architecture to keyword spotting, where the task is to classify short audio snippets into different categories. By partitioning the audio spectrogram into different time windows and applying self-attention, we show that the Keyword Transformer outperforms other network architectures while maintaining a low latency at inference time. Code and pre-trained models will be released soon.
